\documentclass[12pt]{article}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]
\usepackage{authblk}
\usepackage{graphicx}

\bibliographystyle{apalike}

\newcommand{\petibm}{\texttt{PetIBM} }
\newcommand{\petsc}{\texttt{PETSc} }
\newcommand{\cuibm}{\texttt{cuIBM} }
\newcommand{\cusp}{\texttt{CUSP} }
\newcommand{\cuda}{\texttt{CUDA} }
\newcommand{\amgx}{\texttt{AmgX} }
\newcommand{\amgxwrapper}{\texttt{AmgXWrapper} }
\newcommand{\hypre}{\texttt{Hypre} }
\newcommand{\openmpi}{\texttt{OpenMPI} }

\providecommand{\keywords}[1]{\textbf{\textit{Keywords:}} #1}


\begin{document}

\title{Study of the three-dimensional flow around a snake cylinder with Microsoft Azure}

\author[1]{Olivier Mesnard}
\author[1]{Lorena A. Barba}
\affil[1]{Department of Mechanical and Aerospace Engineering,
The George Washington University,
Washington, DC, 20052, United-States}
\maketitle

\begin{abstract}
We use our open-source code \petibm---an immersed boundary method with a fully discrete projection formulation---to perform direct numerical simulations of the three-dimensional flow around a cylinder with an anatomically accurate cross-section of a gliding snake, \textit{Chrysopelea paradisi}, at subcritical Reynolds numbers $1000$ and $2000$.
The simulations ran on the cloud platform Microsoft Azure on instances of the NC-series that features K80 GPU devices.
Our code relies on the \petsc library while the Poisson system is solved on multiple GPU devices with the Nvidia \amgx library.
We report our experience with Microsoft Azure.
We believe that running HPC simulations on a cloud platform such as Microsoft Azure could be a viable alternative to a University cluster.
\end{abstract}

\keywords{Immersed Boundary Method, \petsc, \amgx, Microsoft Azure, Flying snake}

\linenumbers

\section{Introduction}

The immersed boundary method (IBM) became an attractive technique over the last decades to compute the flow around moving and complex geometries.
Peskin\cite{Peskin_1972} proposed this method to solve the flow of a viscous incompressible fluid around flexible heart valves represented as a collection of Lagrangian markers with a restoring force field to enforce the no-slip condition on the immersed boundary.
Since then, the IBM has been widely used and improved to handle rigid boundaries (see \cite{Mittal_Iaccarino_2005} for a comprehensive review).
In this framework, the fluid equations are solved on an Eulerian grid that does not fit onto the surface of the immersed body and the no-slip constraint is enforced by adding a forcing term to the momentum equations.
The method permits to use simple and fixed Cartesian grids, even in the case where the immersed boundary is moving throughout the computational domain.
In 2007, Taira and Colonius\cite{Taira_Colonius_2007} proposed the Immersed-Boundary Projection Method (IBPM), a fractional-step method seen as an approximate block-LU decomposition (\cite{Perot_1993}) where the pressure field and the Lagrangian boundary forces are grouped together to solve a modified-Poisson system to simultaneously satisfy the divergence-free and no-slip conditions on the velocity field.
The boundary is represented by a collection of markers and the transfer between the Lagrangian mesh and the background Eulerian grid is handled by regularized delta functions.
The presence of the immersed boundary augments the size of the Poisson matrix with the presence of off-diagonal terms, making it more expensive to solve with Krylov methods.
\cite{Li_et_al_2016} recently proposed a decoupled version of the IBPM in which an additional block-LU decomposition is performed to further decouple the pressure field from the Lagrangian forces and therefore retrieve a classical Poisson system.

Our research focuses on animal flight and one of our applications deals with the gliding phase of the snake, \textit{Chrysopelea paradisi}\cite{Socha_2011}.
The arboreal reptile jumps from tree branches and starts undulating through the air by sending transversal waves from head to tail.
During the gliding phase, the snake morphs its body cross-section (approximatively circular) by expanding its rib cage to flatten its ventral side, thus turning its entire body into a wing.
\cite{Holden_et_al_2014} experimentally studied the forces generated by a cylindrical body with an anatomically correct cross-section of the gliding snake placed in a freestream flow.
By varying the angle of attack formed by the cross-section with the direction of the freestream flow, Holden and co-workers reported a sudden increase in the lift force for a 35-degree angle at Reynolds number $9000$ and above.
With two-dimensional simulations using \cuibm, \cite{Krishnan_et_al_2014} also reported a lift enhancement for angle of attack 35 degrees (for Reynolds number $2000$ and above); flow visualizations provided an explanation of the lift-enhancement mechanism of the gliding snake.
Furthermore, \cite{Mesnard_Barba_2017} fully replicated the scientific finding of a lift enhancement at Reynolds number $2000$ using four different CFD software.

\petibm is an open-source software that solves the two- and three-dimensional Navier-Stokes equations with an immersed boundary method on distributed-memory architectures with the help of the well-known \petsc library\cite{PETSc_webpage_2017,PETSc_users_manual_2017,Balay_et_al_1997}.
As it is frequent with the projection method, solving the Poisson system iteratively is the bottleneck of our simulations.
We decided to solve the Poisson system on multiple CUDA-capable GPU devices using the NVIDIA library \amgx\cite{Naumov_et_al_2015, Nvidia_AmgX_webpage}.
For that purpose, we developed an interface between \petsc and \amgx, called \amgxwrapper\cite{Chuang_Barba_2017} that handles the data conversion between the two libraries.
The wrapper is no specific to our research-code \petibm and can be used for other \petsc application codes.
For further details about \amgxwrapper, a technical report is available on FigShare\cite{Chuang_Barba_2017_b}.

With the Microsoft Azure Sponsorship program, we obtained a credit of 20000 USD to evaluate the cloud platform for running our in-house CFD software \petibm.
We used instances of the NC-series, that features Nvidia K80 GPU devices, to perform direct numerical simulations of the three-dimensional flow around a cylinder with an anatomically accurate cross-section of the gliding snake.


\section{Simulations Setup}

\subsection{Numerical Method}

We use our in-house software \petibm (version 0.2) to perform direct numerical simulations of the three-dimensional flow in the domain $30c \times 30c \times 3.2c$, where $c$ is the chord length of the snake cross-section of the cylinder centered in the $x$-$y$ plane.
\petibm implements the decoupled IBPM from \cite{Li_et_al_2016} with a fully discrete formulation for the velocity fluxes, the pressure field, and the Lagrangian forces.
The immersed boundary is discretize with the same resolution than the Eulerian mesh on which the Navier-Stokes equations are solved.
We use a structured Cartesian mesh to discretize the domain.
The mesh is uniform in the sub-domain $\left[-0.52c, 3.48c\right]\times\left[2c, 2c\right]\times\left[0, 3.2c\right]$, which contains the immersed boundary, and stretched to the external boundaries with a constant stretching ratio of $1.01$ in all directions.
The snake cylinder is discretized with the same resolution than the background Eulerian mesh on which the Navier-Stokes equations are solved.
We performed simulations on two different meshes.
A ``coarse'' mesh with $1071\times1072\times40$ cells with a grid-width of $0.008c$ in the $x$ and $y$ directions and a grid-width of $0.08c$ in the $z$ direction.
The ``fine'' mesh is obtained by halving the cell width in the uniform sub-domain while maintaining the same stretching ratio outside it.
The ``fine'' mesh contains $1074\times1706\times80$ cells.
A $x$-$z$ slice of the fine mesh has the same resolution than the mesh used for two-dimensional simulations of the same cross-section reported in \cite{Krishnan_et_al_2014} and \cite{Mesnard_Barba_2017}.

Spatial derivatives are computed with second-order central differences.
The convective and diffusive terms are integrated in time with an explicit second-order Adams-Bashforth and an implicit second-order Crank-Nicolson schemes, respectively.

The time-step size was respectively set to $0.001$ and $0.0005$ for the coarse and fine meshes.

The inlet, bottom, and top boundary conditions are set with a uniform velocity of $1$ in the streamwise direction.
We use a linear convective boundary condition at the outlet of the computational domain.
We use periodic boundary conditions in the spanwise direction.

\petibm uses \petsc (version 3.7.4) to compute the solution on distributed-memory architectures.
The system for the velocity fluxes and the system for the Lagrangian boundary forces are solved using a stabilized bi-conjugate gradient method and a GMRES technique, respectively, with a Jacobi preconditioner from the \petsc library.
For both solvers, we used an exit criterion based on an absolute tolerance of $10^{-6}$ for the residuals.
The Poisson system is solved on multiple GPU devices with the Nvidia \amgx library using a conjugate-gradient method preconditioned with a classical algebraic multigrid technique.
The exit criterion for the solver is also based on an absolute tolerance of $10^{-6}$ for the residuals.
We use \amgxwrapper, an open-source interface between \petsc and \amgx to handle the data transfers between the CPUs and the GPU devices.


\subsection{Microsoft Azure}

We used the public cloud platform Microsoft Azure to run simulations of the three-dimensional flow around the snake cylinder.
As mentioned earlier, we solve the Poisson system iteratively on multiple GPU devices.
Thus, increasing the mesh grid requires more memory on device.
Our University cluster, Colonial One, possesses nodes with CUDA-capable GPU devices (2 K20 devices per node).
For the simulation on the finest grid (233 million cells), we estimated we would need at least 24 nodes on Colonial One to satisfy the memory requirements for solving the Poisson system.
For sure, we would have to wait a considerable amount of time in the queue before the job starts.
Moreover, the runtime of those simulations would have had easily exceeded the the 7-day time limit imposed by the cluster.
Looking for an alternative to run our "big" simulations, we considered public clouds.
In 2016, we obtained a Microsoft Azure Sponsorship for a credit of $20,000$ to evaluate their platform and run our MPI-based application, PetIBM, to study the three-dimensional flow around a cylinder with an anatomically accurate cross-section of the flying snake.

We started by computing 100 time units of flow simulation on the coarse from a flow initially set to the freestream flow.
The simulation ran on one instance NC24 (24 CPU cores and 4 Nvidia K80 GPU devices).
From there, the numerical solution was interpolated on the fine mesh and we ran the simulation for 90 additional time units using 4 NC24r instances using 24 processes and 4 Nvidia K80 GPU devices per node and taking advantage of the low-latency InfiniBand network.

Finally, we ran two additional simulation on the coarse mesh restarting from the numerical solution obtained at Reynolds number $2000$ and angle of attack $35^o$ by either reducing the angle of attack to $30^o$ or decreasing the Reynolds number down to $1000$.
For each of those additional run, we computed another 100 time units using 2 instances NC24r with 24 CPU processes and 4 Nvidia K80 GPU devices per node.

The first simulation (Reynolds number $2000$ and angle of attack $35^o$ on the coarse mesh) was run by spinning one NC24 instance in the South Central US region using the Ubuntu 16.04 image published by Canonical.
\petibm-0.2 was installed along with \petsc-3.7.4, \openmpi-1.8.8, \cuda-6.5, \amgx-1.2-build108.
At the time of running this simulation, \amgx was available as a compiled library to Nvidia registered developers coming a free license for non-commercial use\footnote{\url{https://developer.nvidia.com/amgx}}.
At the time of writing, \amgx is now an open-source project hosted on GitHub\footnote{\url{https://github.com/NVIDIA/AMGX}}.

The simulation on the fine mesh as well as the two additional simulations on the coarse mesh ran on NC24r instances in the East US region using Azure Batch service and Batch Shipyard.
For those runs, we used CentOS-7 base image with \petibm-0.2, \petsc-3.7.4, Intel MPI, \cuda-8.0 ,and \amgx-2.0 (source code provided by Nvidia under NDA so we could build the library with Intel MPI).

Azure Batch is a platform service for running HPC applications in the cloud that relieves the user from manually creating, configuring, and managing a HPC cluster, virtual machines, virtual networks, job and task scheduling infrastructure.
The user creates a pool in a Batch account, a job to run the workload on the pool, and add tasks to the job.
Batch service automatically schedules the tasks for execution on the compute nodes present in the pool.
Each task uses the application uploaded to process the input files present on a Azure Storage account.
(Note that a Batch account comes with a default limit of dedicated cores and to run applications on a larger number, the user needs to contact the Azure support team to increase the quota.)
With Batch service, you can even configure the pool to run tasks in Docker containers.
(The container images are copied to the VMs when the pool is created.)
Running applications in Docker containers removes the constraint from the user in dealing with software dependencies on the cloud.
For our PetIBM application, we built the container on our local machine using Nvidia Docker, uploaded the image to DockerHub in private registry, and ran our simulation tasks in containers with Batch service.

To make our workflow with Batch service easier, we also used Batch Shipyard, an open-source software, hosted on GitHub, mainly developed by Microsoft Azure, to help us provision and execute container-based tasks on Azure Batch compute nodes.
With a few configuration files and a few command lines in the terminal, Batch Shipyard creates a pool of compute nodes, submit a job with Azure Batch, and run multiple tasks with Azure Batch service.
With Batch Shipyard, the user does not have to master the Batch .NET SDK to create a pool of compute nodes that supports running container-based tasks.
The process of using the Batch .NET SDK for such task is extensively documented on the official Microsoft Azure documentation; however, the learning curve is definitely steeper and longer than when adopting Batch Shipyard.
With Batch Shipyard, we have been able to run container-based multi-instance tasks for our PetIBM simulations on the NC24r instances (featuring Nvidia K80 GPU devices) with Azure Linux VMs and over the low-latency and high-bandwidth Infiniband/RDMA network.
As mentioned at the end of the documentation for Batch Shipyard, only Intel MPI can be used in conjunction with Infiniband/RDMA on Azure Linux VMs; this is a current limitation of the underlying VM and host drivers.
At that time, Nvidia AmgX was not yet an open-source project hosted GitHub and the shared library, provided with a limited free-to-use license to CUDA-registered developers, was built with OpenMPI.
Understanding our problem, folks at Nvidia gave us access to the source code under a non-disclosure agreement so we could build the library with Intel MPI.
This is the only reason why the Docker image with PetIBM and its dependencies was made private on DockerHub.
Finally, we used Microsoft Azure CLI 2.0 to transfer data between our local machine and an Azure Storage account.


\section{Results}

\subsection{MPI Communication Benchmarks}

We compared the MPI communication performance between Microsoft Azure and Colonial One.
We used the micro-benchmarks from the Ohio State University (\url{http://mvapich.cse.ohio-state.edu/benchmarks}, version 5.3.2) to perform point-to-point tests and evaluate the latency and bandwidth on the two clusters. We used two nodes (connection over InfiniBand network) on each platform and repeated each test 5 times; the mean latencies and bandwidths are reported in Figure \ref{latency_bandwidth_colonialone_azure}. As the two platforms have identical hardware specifications, we observe similar performances.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/latencyBandwidthColonialOneAzure.png}
\caption{Point-to-point latency (left) and bandwidth (right) obtained on Colonial One, between two A9 nodes, and between two NC24r nodes of Microsoft Azure.}
\label{latency_bandwidth_colonialone_azure}
\end{figure}

\subsection{Poisson Benchmarks}

We also ran a Poisson benchmark on a mesh-grid with 46M cells solving the system with a conjugate-gradient method from PETSc preconditioned by a classical multigrid technique from \hypre BoomerAMG.
Figure \ref{poisson_colonialone_azurea9} reports the time-to-solution for the Poisson system (averaged over 5 repetitions) when solved on up to 8 compute nodes.
These results confirms that we are able to get similar performances on a public cloud compared to our local University cluster.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/poissonScalingColonialOneAzureA9.png}
\caption{Run-times to solve a Poisson system with 46 millions of unknowns as a function of the number of the compute nodes. We compare run-times obtained on Colonial One and on Microsoft Azure (A9 instances).}
\label{poisson_colonialone_azurea9}
\end{figure}

\subsection{Flying Snake to the Cloud}

We computed $100$ non-dimensional time units of flow simulation ($100000$ time steps) on the coarse mesh at Reynolds number $2000$ and angle of attack $35^o$.
From there, we interpolated the final numerical solution onto the fine mesh and restarted time simulation.
The simulation ran for an additional $98614$ time steps (time $149.307$), at which point the simulation was no longer running because of the 7-day time limit enforced on Batch tasks; so, we killed it.
The simulation on the finer mesh was restarted from time $147.6$ and run until time $175.2$.
Finally, it was restarted one last time $174.8$ until time $190.8$.

The drag and Lift coefficients are defined as

\begin{equation}
	C_D = \frac{F_x}{\frac{1}{2} \rho U_\infty^2 c L_z},\quad C_L = \frac{F_y}{\frac{1}{2} \rho U_\infty^2 c L_z}
\end{equation}

\noindent where $F_x$ and $F_y$ are the total forces in the streamwise and cross-flow directions, respectively, $c$ is the chord length of the snake cross-section, and $L_z$ is the spanwise length.

Figure \ref{force_coefficients_3d2k35_azure} shows the instantaneous force coefficients obtained on the 3D snake cylinder.
We add previous 2D results obtained with the same code (and with a similar mesh resolution) for comparison.
During the initial transient period, the 2D and 3D forces are similar as the 3D structures are not yet present in the flow.
After that, the aerodynamic force coefficients acting on the 3D cylinder are smaller than those obtained with a 2D snake profile.
\cite{Krishnan_et_al_2014} chose to average the force coefficients between $32$ and $64$ time units of flow simulation.
Doing the same for the 2D results, we obtain a mean drag coefficient of $1.22$ and a mean lift coefficient of $1.95$.
For the 3D runs, we decided to average the forces coefficients between $50$ and $150$ time units.
Compared to the 2D results, the mean drag coefficient is relatively $32.7\%$ smaller ($0.82$) and the mean lift coefficient is relatively $20.4$ smaller.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/forceCoefficients3d2k35.png}
\caption{Drag coefficient (top) and lift coefficient (bottom) obtained on the coarse and fine meshes for the three-dimensional simulations at Reynolds number $2000$ and angle of attack $35^o$. Previously obtained force coefficients for the two-dimensional simulation are added for comparison.}
\label{force_coefficients_3d2k35_azure}
\end{figure}

\section{Discussion}

Not everything went smooth when run our task with Azure Batch service.
The simulation of the 3D flow around the snake cylinder on a mesh with 233 million cells is a long run that was estimated to complete in about 3 weeks.
However, 10 days after the run was submitted with Batch Shipyard to Azure Batch (a job was submitted in a pool of 4 NC24r nodes using 24 CPU processes and 4 K80 GPU devices per node), we noticed something went wrong: the numerical solution was not anymore written into files (we output the aerodynamic forces at every time step of the simulation) and the nodes were tagged as "IDLE" on the Microsoft Azure portal while the Batch pool and the job's task were still respectively marked as "active" and "running".
(Logging into one of the compute nodes, we saw that the processes were still running.)
Fred Park, software engineer at Microsoft, told us that Batch tasks have a 7-day time limit enforced (from the moment tasks are submitted), a restriction that, at the time of submission had not yet been added to the official Azure Batch documentation and was unknown to us.
However, it is mentioned in the "General Limitations and Restrictions" at the end of the Batch Shipyard documentation and we did not see it.
Still, this is not the first time that a user is not reading thoroughly the entire documentation when approaching a software.
But in the end, we did not get any notification about our job being killed when it exceeded 7 days of runtime.
In the end, it took us (the users) almost three days to realize that something went wrong and that we needed to kill the job and restart the simulation (from the last time-step solution saved): a time lapse that cost us about $1,800$ on our Microsoft Azure Sponsorship.

In the process of developing and running PetIBM simulations on Microsoft Azure, we have to admit that our open-source and reproducibility standards took a hit.
We used a private registry to push our PetIBM image on DockerHub because it contains the source code of Nvidia AmgX, which was not open-source at that point.
The private image was used to run the simulations in a Docker container with Azure Batch service and Batch Shipyard.
In addition to that, the long simulation runtimes (about three weeks for the simulation of the finest mesh) are expensive, making it more difficult for other to reproduce the results.
At least, we took the precaution to make public all configuration files, pre- and post-processing scripts on GitHub where we also detail the workflow used.

\bibliography{references}

\end{document}