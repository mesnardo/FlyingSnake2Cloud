\documentclass[12pt]{article}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]
\usepackage{authblk}
\usepackage{graphicx}

\bibliographystyle{apalike}

\newcommand{\petibm}{\texttt{PetIBM} }
\newcommand{\petsc}{\texttt{PETSc} }
\newcommand{\cuibm}{\texttt{cuIBM} }
\newcommand{\cusp}{\texttt{CUSP} }
\newcommand{\cuda}{\texttt{CUDA} }
\newcommand{\amgx}{\texttt{AmgX} }
\newcommand{\amgxwrapper}{\texttt{AmgXWrapper} }
\newcommand{\hypre}{\texttt{Hypre} }
\newcommand{\openmpi}{\texttt{OpenMPI} }

\providecommand{\keywords}[1]{\textbf{\textit{Keywords:}} #1}


\begin{document}

\title{Study of the three-dimensional flow around a snake cylinder with Microsoft Azure}

\author[1]{Olivier Mesnard}
\author[1]{Lorena A. Barba}
\affil[1]{Mechanical and Aerospace Engineering, The George Washington University, \\
Washington, DC, 20052, United-States}
\maketitle

\begin{abstract}
We use our open-source code \petibm---an immersed boundary method with a fully discrete projection formulation---to perform direct numerical simulations of the three-dimensional flow around a cylinder with an anatomically accurate cross-section of a gliding snake, \textit{Chrysopelea paradisi}, at subcritical Reynolds numbers $1000$ and $2000$.
The simulations ran on the cloud platform Microsoft Azure on instances of the NC-series that features K80 GPU devices.
Our code relies on the \petsc library while the Poisson system is solved on multiple GPU devices with the Nvidia \amgx library.
We report our experience with Microsoft Azure.
We believe that running HPC simulations on a cloud platform such as Microsoft Azure could be a viable alternative to a University cluster.
\end{abstract}

\keywords{Immersed Boundary Method, \petsc, \amgx, Microsoft Azure, Flying snake}

\linenumbers

\section{Introduction}

The immersed boundary method (IBM) became an attractive technique over the last decades to compute the flow around moving and complex geometries.
Peskin\cite{Peskin_1972} proposed this method to solve the flow of a viscous incompressible fluid around flexible heart valves represented as a collection of Lagrangian markers with a restoring force field to enforce the no-slip condition on the immersed boundary.
Since then, the IBM has been widely used and improved to handle rigid boundaries (see \cite{Mittal_Iaccarino_2005} for a comprehensive review).
In this framework, the fluid equations are solved on an Eulerian grid that does not fit onto the surface of the immersed body and the no-slip constraint is enforced by adding a forcing term to the momentum equations.
The method permits to use simple and fixed Cartesian grids, even in the case where the immersed boundary is moving throughout the computational domain.
In 2007, Taira and Colonius\cite{Taira_Colonius_2007} proposed the Immersed-Boundary Projection Method (IBPM), a fractional-step method seen as an approximate block-LU decomposition (\cite{Perot_1993}) where the pressure field and the Lagrangian boundary forces are grouped together to solve a modified-Poisson system to simultaneously satisfy the divergence-free and no-slip conditions on the velocity field.
The boundary is represented by a collection of markers and the transfer between the Lagrangian mesh and the background Eulerian grid is handled by regularized delta functions.
The presence of the immersed boundary augments the size of the Poisson matrix with the presence of off-diagonal terms, making it more expensive to solve with Krylov methods.
\cite{Li_et_al_2016} recently proposed a decoupled version of the IBPM in which an additional block-LU decomposition is performed to further decouple the pressure field from the Lagrangian forces and therefore retrieve a classical Poisson system.

Our research focuses on animal flight and one of our applications deals with the gliding phase of the snake, \textit{Chrysopelea paradisi}\cite{Socha_2011}.
The arboreal reptile jumps from tree branches and starts undulating through the air by sending transversal waves from head to tail.
During the gliding phase, the snake morphs its body cross-section (approximatively circular) by expanding its rib cage to flatten its ventral side, thus turning its entire body into a wing.
\cite{Holden_et_al_2014} experimentally studied the forces generated by a cylindrical body with an anatomically correct cross-section of the gliding snake placed in a freestream flow.
By varying the angle of attack formed by the cross-section with the direction of the freestream flow, Holden and co-workers reported a sudden increase in the lift force for a 35-degree angle at Reynolds number $9000$ and above.
With two-dimensional simulations using \cuibm, \cite{Krishnan_et_al_2014} also reported a lift enhancement for angle of attack 35 degrees (for Reynolds number $2000$ and above); flow visualizations provided an explanation of the lift-enhancement mechanism of the gliding snake.
Furthermore, \cite{Mesnard_Barba_2017} fully replicated the scientific finding of a lift enhancement at Reynolds number $2000$ using four different CFD software.

\petibm is an open-source software that solves the two- and three-dimensional Navier-Stokes equations with an immersed boundary method on distributed-memory architectures with the help of the well-known \petsc library\cite{PETSc_webpage_2017,PETSc_users_manual_2017,Balay_et_al_1997}.
As it is frequent with the projection method, solving the Poisson system iteratively is the bottleneck of our simulations.
We decided to solve the Poisson system on multiple CUDA-capable GPU devices using the NVIDIA library \amgx\cite{Naumov_et_al_2015, Nvidia_AmgX_webpage}.
For that purpose, we developed an interface between \petsc and \amgx, called \amgxwrapper\cite{Chuang_Barba_2017} that handles the data conversion between the two libraries.
The wrapper is no specific to our research-code \petibm and can be used for other \petsc application codes.
For further details about \amgxwrapper, a technical report is available on FigShare\cite{Chuang_Barba_2017_b}.

With the Microsoft Azure Sponsorship program, we obtained a credit of 20000 USD to evaluate the cloud platform for running our in-house CFD software \petibm.
We used instances of the NC-series, that features Nvidia K80 GPU devices, to perform direct numerical simulations of the three-dimensional flow around a cylinder with an anatomically accurate cross-section of the gliding snake.


\section{Simulation Setup}

We use our in-house software \petibm (version 0.2) to perform direct numerical simulations of the three-dimensional flow in the domain $30c \times 30c \times 3.2c$ where $c$ is the chord length of the snake cross-section of the cylinder that is centered in the $x$-$y$ plane.
\petibm implements the decoupled IBPM from \cite{Li_et_al_2016} with a fully discrete formulation for the velocity fluxes, the pressure field, and the Lagrangian forces.
The immersed boundary is discretize with the same resolution than the Eulerian mesh on which the Navier-Stokes equations are solved.
We use a structured Cartesian mesh to discretize the domain.
The mesh is uniform in the sub-domain $\left[-0.52c, 3.48c\right]\times\left[2c, 2c\right]\times\left[0, 3.2c\right]\times$, which contains the immersed boundary, and stretched to the external boundaries with a constant stretching ratio of $1.01$ in all directions.
The snake cylinder is discretized with the same resolution than the background Eulerian mesh on which the Navier-Stokes equations are solved.
We performed simulations on two different meshes.
A ``coarse'' mesh with $1071\times1072\times40$ cells with a grid-width of $0.008c$ in the $x$ and $y$ directions and a grid-width of $0.08c$ in the $z$ direction.
The ``fine'' mesh is obtained by halving the cell width in the uniform sub-domain while maintaining the same stretching ratio outside it.
The ``fine'' mesh contains $1074\times1706\times80$ cells.
A $x$-$z$ slice of the fine mesh has the same resolution than the mesh used for two-dimensional simulations of the same cross-section reported in \cite{Krishnan_et_al_2014} and \cite{Mesnard_Barba_2017}.

Spatial derivatives are computed with second-order central differences.
The convective and diffusive terms are integrated in time with an explicit second-order Adams-Bashforth and an implicit second-order Crank-Nicolson schemes, respectively.

The time-step size was respectively set to $0.001$ and $0.0005$ for the coarse and fine meshes.

The inlet, bottom, and top boundary conditions are set with a uniform velocity of $1$ in the streamwise direction.
We use a linear convective boundary condition at the outlet of the computational domain.
We use periodic boundary conditions in the spanwise direction.

\petibm uses \petsc (version 3.7.4) to compute the solution on distributed-memory architectures.
The system for the velocity fluxes and the system for the Lagrangian boundary forces are solved using a stabilized bi-conjugate gradient method and a GMRES technique, respectively, with a Jacobi preconditioner from the \petsc library.
For both solvers, we used an exit criterion based on an absolute tolerance of $10^{-6}$ for the residuals.
The Poisson system is solved on multiple GPU devices with the Nvidia \amgx library using a conjugate-gradient method preconditioned with a classical algebraic multigrid technique.
The exit criterion for the solver is also based on an absolute tolerance of $10^{-6}$ for the residuals.
We use \amgxwrapper, an open-source interface between \petsc and \amgx to handle the data transfers between the CPUs and the GPU devices.

We started by computing 100 time units of flow simulation on the coarse from a flow initially set to the freestream flow.
The simulation ran on one instance NC24 (24 CPU cores and 4 Nvidia K80 GPU devices).
From there, the numerical solution was interpolated on the fine mesh and we ran the simulation for 90 additional time units using 4 NC24r instances using 24 processes and 4 Nvidia K80 GPU devices per node and taking advantage of the low-latency InfiniBand network.

Finally, we ran two additional simulation on the coarse mesh restarting from the numerical solution obtained at Reynolds number $2000$ and angle of attack $35^o$ by either reducing the angle of attack to $30^o$ or decreasing the Reynolds number down to $1000$.
For each of those additional run, we computed another 100 time units using 2 instances NC24r with 24 CPU processes and 4 Nvidia K80 GPU devices per node.

The first simulation (Reynolds number $2000$ and angle of attack $35^o$ on the coarse mesh) was run by spinning one NC24 instance in the South Central US region using the Ubuntu 16.04 image published by Canonical.
\petibm-0.2 was installed along with \petsc-3.7.4, \openmpi-1.8.8, \cuda-6.5, \amgx-1.2-build108.
At the time of running this simulation, \amgx was available as a compiled library to Nvidia registered developers coming a free license for non-commercial use\footnote{\url{https://developer.nvidia.com/amgx}}.
At the time of writing, \amgx is now an open-source project hosted on GitHub\footnote{\url{https://github.com/NVIDIA/AMGX}}.

The simulation on the fine mesh as well as the two additional simulations on the coarse mesh ran on NC24r instances in the East US region using Azure Batch service and Batch Shipyard.
For those runs, we used CentOS-7 base image with \petibm-0.2, \petsc-3.7.4, Intel MPI, \cuda-8.0 ,and \amgx-2.0 (source code provided by Nvidia under NDA so we could build the library with Intel MPI).


\section{Results}

\subsection{MPI Communication Benchmarks}

We compared the MPI communication performance between Microsoft Azure and Colonial One.
We used the micro-benchmarks from the Ohio State University (\url{http://mvapich.cse.ohio-state.edu/benchmarks}, version 5.3.2) to perform point-to-point tests and evaluate the latency and bandwidth on the two clusters. We used two nodes (connection over InfiniBand network) on each platform and repeated each test 5 times; the mean latencies and bandwidths are reported in Figure \ref{latency_bandwidth_colonialone_azure}. As the two platforms have identical hardware specifications, we observe similar performances.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/latencyBandwidthColonialOneAzure.png}
\caption{Point-to-point latency (left) and bandwidth (right) obtained on Colonial One, between two A9 nodes, and between two NC24r nodes of Microsoft Azure.}
\label{latency_bandwidth_colonialone_azure}
\end{figure}

\subsection{Poisson Benchmarks}

We also ran a Poisson benchmark on a mesh-grid with 46M cells solving the system with a conjugate-gradient method from PETSc preconditioned by a classical multigrid technique from \hypre BoomerAMG.
Figure \ref{poisson_colonialone_azurea9} reports the time-to-solution for the Poisson system (averaged over 5 repetitions) when solved on up to 8 compute nodes.
These results confirms that we are able to get similar performances on a public cloud compared to our local University cluster.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/poissonScalingColonialOneAzureA9.png}
\caption{Run-times to solve a Poisson system with 46 millions of unknowns as a function of the number of the compute nodes. We compare run-times obtained on Colonial One and on Microsoft Azure (A9 instances).}
\label{poisson_colonialone_azurea9}
\end{figure}

\subsection{Flying Snake to the Cloud}


\section{Conclusion}

To be added.

\bibliography{references}

\end{document}