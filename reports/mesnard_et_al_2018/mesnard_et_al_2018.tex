\documentclass[10pt]{article}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]
\usepackage{authblk}
\usepackage{graphicx}

\bibliographystyle{apalike}

\newcommand{\petibm}{\texttt{PetIBM} }
\newcommand{\petsc}{\texttt{PETSc} }
\newcommand{\cuibm}{\texttt{cuIBM} }
\newcommand{\cusp}{\texttt{CUSP} }
\newcommand{\amgx}{\texttt{AmgX} }
\newcommand{\amgxwrapper}{\texttt{AmgXWrapper} }
\newcommand{\hypre}{\texttt{Hypre} }

\providecommand{\keywords}[1]{\textbf{\textit{Keywords:}} #1}


\begin{document}

\title{Using AmgX to accelerate a PETSc-based Immersed Boundary Method code}

\author[1]{Olivier Mesnard}
\author[1]{Pi-Yueh Chuang}
\author[1]{Lorena A. Barba}
\affil[1]{Mechanical and Aerospace Engineering, The George Washington University, \\
Washington, DC, 20052, United-States}
\maketitle

\begin{abstract}
Our open-source code \petibm---an immersed boundary method with a fully discrete projection formulation---was written to take advantage of the \petsc library for solving the Poisson system. 
We have now added the capacity to accelerate the time to solution on CUDA-capable GPU devices using the Nvidia library \amgx.
To provide access to \amgx solvers from a \petsc-based code, we developed a wrapper code that converts the data structures between the two libraries.
This wrapper code could be useful to other \petsc applications that want to use GPUs via \amgx.
Our application of interest is the three-dimensional flow around a  flying-snake,  to reveal the lift-enhancement mechanisms used by this unconventional glider.
We are developing capability to study this problem in Microsoft Azure cloud services.
\end{abstract}

\keywords{Immersed Boundary Method, \petsc, \amgx, Microsoft Azure, Flying snake}

\linenumbers

\section{Introduction}

The immersed boundary method (IBM) became an attractive technique over the last decades to compute the flow around moving and complex geometries.
Peskin\cite{Peskin_1972} proposed this method to solve the flow of a viscous incompressible fluid around flexible heart valves represented as a collection of Lagrangian markers with a restoring force field to enforce the no-slip condition on the immersed boundary.
Since then, the IBM has been widely used and improved to handle rigid boundaries (see \cite{Mittal_Iaccarino_2005} for a comprehensive review).
In this framework, the fluid equations are solved on an Eulerian grid that does not fit onto the surface of the immersed body and the no-slip constraint is enforced by adding a forcing term to the momentum equations.
The method permits to use simple and fixed Cartesian grids, even in the case where the immersed boundary is moving throughout the computational domain.
In 2007, Taira and Colonius\cite{Taira_Colonius_2007} proposed the Immersed-Boundary Projection Method (IBPM), a fractional-step method seen as an approximate block-LU decomposition (\cite{Perot_1993}) where the pressure field and the Lagrangian boundary forces are grouped together to solve a modified-Poisson system to simultaneously satisfy the divergence-free and no-slip conditions on the velocity field.
The boundary is represented by a collection of markers and the transfer between the Lagrangian mesh and the background Eulerian grid is handled by regularized delta functions.
The presence of the immersed boundary augments the size of the Poisson matrix with the presence of off-diagonal terms, making it more expensive to solve with Krylov methods.

One of our in-house software, called \cuibm\cite{Krishnan_et_al_2017}, implements the IBPM, solving the two-dimensional Navier-Stokes equations on a single CUDA-capable GPU device with the open-source NVIDIA library \cusp\cite{Cusp}.
Our research focuses on animal flight and one of our applications deals with the gliding phase of the snake, \textit{Chrysopelea paradisi}\cite{Socha_2011}.
The arboreal reptile jumps from tree branches and starts undulating through the air by sending transversal waves from head to tail.
During the gliding phase, the snake morphs its body cross-section (approximatively circular) by expanding its rib cage to flatten its ventral side, thus turning its entire body into a wing.
\cite{Holden_et_al_2014} experimentally studied the forces generated by a cylindrical body with an anatomically correct cross-section of the gliding snake placed in a freestream flow.
By varying the angle of attack formed by the cross-section with the direction of the freestream flow, Holden and co-workers reported a sudden increase in the lift force for a 35-degree angle at Reynolds number $9000$ and above.
With two-dimensional simulations using \cuibm, \cite{Krishnan_et_al_2014} also reported a lift enhancement for angle of attack 35 degrees (for Reynolds number $2000$ and above); flow visualizations provided an explanation of the lift-enhancement mechanism of the gliding snake.

With \cuibm working on a single GPU device, the largest mesh size that can be run depends of the size of the memory on device.
To tackle larger problems, we developed another IBM software, called \petibm, that solves the two- and three-dimensional Navier-Stokes with the same IBM.
\petibm works on distributed-memory architectures, using the open-source library \petsc\petsc\cite{PETSc_webpage_2017,PETSc_users_manual_2017,Balay_et_al_1997}.
As it is frequent with the projection method, solving the Poisson system iteratively is the bottleneck of our simulations.
We decided to solve the Poisson system on multiple CUDA-capable GPU devices using the NVIDIA library \amgx\cite{Naumov_et_al_2015, Nvidia_AmgX_webpage}.
For that purpose, we developed an interface between \petsc and \amgx that is not specific to our in-house software and can be used by \petsc users who wants to accelerate the time-to-solution of the linear systems.

The next section describes the flow solver, \petibm\cite{PetIBM}. We next present how we interfaced our \petsc-based code with the NVidia library \amgx to offload the resolution of the Poisson system on multiple CUDA-capable GPU devices.
As we deal with problems that are larger in size, it requires more computational resources and longer time in the queue of HPC clusters.
We finally looked into the alternative of Cloud computing to run our simulations thanks to a sponsorship from Microsoft Azure.


\section{\amgxwrapper: an interface between \petsc and \amgx}

Our in-house software \petibm heavily relies on the data structure, parallel routines, and linear solvers of the \petsc library.
This allows us to hide a lot of MPI implementations from the users and to rapidly implement new things working on distributed-memory architecture.

In first place, we implemented the IBPM, as it was done in \cuibm.
With this projection method, at each time step, we solve a linear for the velocity fluxes and, then, solve a modified-Poisson system for the couple pressure field/boundary Lagrangian forces to enforce the constraints of incompressibility and no-slip.
As commonly observed with the projection method, solving the Poisson system iteratively takes most of the runtime.
This is even worse in the presence of an immersed boundary where the Poisson matrix becomes larger and contains off-diagonal terms.
As an example, we ran a two-dimensional snake simulation (35-degree angle-of-attack and Reynolds number $2000$) over $80$ non-dimensional time units ($200000$ time steps) on a mesh with 2.9 million cells ($1704\times1704$).
Solving the modified-Poisson system took about $90\%$ of the total runtime with a Conjugate-gradient technique preconditioned by an algebraic multigrid preconditioner (aggregation) from the \petsc library on one compute node of our University HPC cluster with $16$ CPU cores (Dual 8-Core 2.6GHz Intel Xeon E5-2670).

Here, our objective is to reduce the time-to-solution for the modified-Poisson system by using the NVIDIA \amgx library.
\amgx provides access to various Krylov solvers and different algebraic multigrid techniques with the possibility to solve systems on multiple CUDA-capable GPU devices located on a single node or across multiple nodes.
At the time of running the experiments, \amgx was available with a free license for non-commercial use\footnote{\url{https://developer.nvidia.com/amgx}}.
At the time of writing, \amgx is now an open-source project hosted on GitHub\footnote{\url{https://github.com/NVIDIA/AMGX}}.

The \petsc data structures are different from the \amgx as they target different hardware.
Thus, we decided to develop \amgxwrapper\cite{Chuang_Barba_2017}, a simple interface between \amgx and \petsc, that handles the data conversion between the two libraries.
We tried to make it simple and not specific to our code PetIBM, so that a user would just have to initialize, set the matrix (with a \petsc object), solve the system at each time step (using \petsc vectors), then finalize to release the memory.
Note that this wrapper can used for other \petsc application codes.
For further details about \amgxwrapper, a technical report is available on FigShare\cite{Chuang_Barba_2017_b}.

\section{Results}

We used our two-dimensional snake application on a mesh-grid with 2.9 millions cells to report speedup in PetIBM using AmgX through our wrapper.
In all runs shown in Figure \ref{flying_snake_piyueh}, the modified-Poisson system was solved using a conjugate-gradient method preconditioned by an algebraic multigrid technique with aggregation.
CPU cases shown on the left part were run using \petsc with 12 CPU cores per node.
GPU cases at the center and right parts of the figure were run using \amgx with 12 CPU cores and 2 K20 devices per node.
For CPU runs, we obtained good scaling as we increase the number of nodes but still, we see that the Poisson solver takes most of the runtime.
With GPU computing, we already observed a 21-time speedup when comparing 1 GPU node and 1 CPU node.
We even obtained remarkable speed-up on one of our workstations.
With \amgx and our wrapper, we have been able to considerably reduce the time-to-solution of the Poisson system.
But as said before, because the pressure and the boundary forces are coupled in this projection method, the modified-Poisson system is expensive to solve.
So, we also tried to use a new immersed-boundary technique.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/flyingSnakePiYueh.png}
\caption{Run-times for the two-dimensional flying snake case (angle of attack 35 degrees and Reynolds number $2000$), using \petibm and \amgx with \amgxwrapper.}
\label{flying_snake_piyueh}
\end{figure}

In order to further reduce the time-to-solution for the Poisson system, we implemented a decoupled version of the IBPM \cite{Li_et_al_2016} in \petibm.
The method applies a second block-LU decomposition to decouple the pressure field from the boundary forces.
At each time step of the computation, we solve a system for the velocity fluxes, then solve a traditional Poisson system for the pressure to enforce the divergence-free constraint.
Finally, we impose the no-slip condition on the velocity at the boundary by solving a small system for the Lagrangian forces.
Therefore, by the end of each time step, the velocity is not exactly divergence-free.
However, Figure \ref{force_coefficients_snake_2k30} shows that the history of aerodynamic forces seems to be independent with respect of the IBM used, which means that, for our application, it is not necessary to satisfy both constraints simultaneously.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/forceCoefficients2k30.png}
\caption{History of the lift (top) and drag (bottom) coefficients on the snake cross-section at Reynolds number $2000$ and angle of attack $30^o$. We used a conjugate-gradient technique preconditioned by an algebraic multigrid technique from the \amgx library to solve the Poisson system. We used a classical version of the preconditioner for the Poisson matrix and an aggregation version for the modified-Poisson matrix.}
\label{force_coefficients_snake_2k30}
\end{figure}

Figure \ref{walltime_snake_2k30} reports the wall-times obtained with the two IBMs implemented in \petibm for runs with and without \amgx.
All simulations reported were run over 80 non-dimensional time units and the Poisson system (or its modified version) was solved using a conjugate-gradient method preconditioned with an algebraic multigrid technique.
The exit criterion for the iterative solvers is set to $10^{-6}$, based on the absolute tolerance for the residuals.
Simulations without \amgx (first and third columns) ran on a single compute node with 16 CPU cores (Dual 8-Core 2.6GHz Intel Xeon E5-2670) and with \petsc GAMG and \hypre BoomerAMG for the preconditioner with the IBPM and the decoupled method, respectively.
Simulations with \amgx (second and last columns) ran on a single compute node with 12 CPU cores (Dual 6-Core 2.0GHz Intel Xeon E5-2620) and 2 GPU devices (NVIDIA Tesla K20) and with an aggregation and a classical variant for the multigrid technique for the IBPM and its decoupled version, respectively.
By enabling GPU computing with \amgx, we observe a 2.4-time speedup in our simulation.
More interestingly, with the decoupled method and AmgX, we now report a 6-time speedup and solving the Poisson system iteratively is not anymore the bottleneck in our simulations.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/walltimes2k30.png}
\caption{Wall-times for the two-dimensional snake simulation (30-degree angle of attack and Reynolds number $2000$) with the IBPM and its decoupled version. We also compare wall-times between CPU runs (with \petsc and \hypre) and GPU runs (with \amgx and \amgxwrapper).}
\label{walltime_snake_2k30}
\end{figure}


\section{Flying snakes to the cloud}

Simulations reported in the previous section were run on Colonial One, our HPC cluster at the George Washington University that feature CPU and GPU nodes (with NVIDIA Tesla K20 devices).
Having access to this cluster is a privilege, but sometimes maintenance issues slow down your progress and waiting in queue for an unknown period of time can be frustrating.
Thus, we wanted to look for an alternative to Colonial One.
In 2016, we received a sponsorship from Microsoft Azure\footnote{\url{www.azure4research.com}} for 20,000 CPU hours to be able to run our code on their public cloud platform.
On Microsoft Azure, we have A9 instances (Dual 8-Core 2.6GHz Intel Xeon E5-2670) and instances of the NC series that features NVIDIA Tesla K80 GPU devices (the biggest instance of the NC series gives access to 4 GPU devices).

\subsection{MPI Communication Benchmarks}

We compared the MPI communication performance between Microsoft Azure and Colonial One.
We used the micro-benchmarks from the Ohio State University (\url{http://mvapich.cse.ohio-state.edu/benchmarks}, version 5.3.2) to perform point-to-point tests and evaluate the latency and bandwidth on the two clusters. We used two nodes (connection over InfiniBand network) on each platform and repeated each test 5 times; the mean latencies and bandwidths are reported in Figure \ref{latency_bandwidth_colonialone_azure}. As the two platforms have identical hardware specifications, we observe similar performances.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/latencyBandwidthColonialOneAzure.png}
\caption{Point-to-point latency (left) and bandwidth (right) obtained on Colonial One and a cluster of two A9 nodes of Microsoft Azure.}
\label{latency_bandwidth_colonialone_azure}
\end{figure}

\subsection{Poisson benchmark}

We also ran a Poisson benchmark on a mesh-grid with 46M cells solving the system with a conjugate-gradient method from PETSc preconditioned by a classical multigrid technique from \hypre BoomerAMG.
Figure \ref{poisson_colonialone_azure} reports the time-to-solution for the Poisson system (averaged over 5 repetitions) when solved on up to 8 compute nodes.
These results confirms that we are able to get similar performances on a public cloud compared to our local University cluster.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/poissonScalingColonialOneAzure.png}
\caption{Run-times to solve a Poisson system with 46 millions of unknowns as a function of the number of the compute nodes. We compare run-times obtained on Colonial One and on Microsoft Azure (A9 instances).}
\label{poisson_colonialone_azure}
\end{figure}

\subsection{Two-dimensional flying snake}

Using \petibm and \amgx (interfaced with \amgxwrapper), we aim to see if we can reduce our cloud-computing expenses with the Microsoft Azure platform.

For that purpose, we started with the two-dimensional snake example as a test case.
The cross-section is centered in a $30c \times 30c$ domain (with $c$, the chord length of the immersed body) and forms a 35-degree angle of attack from the freestream direction.
The Reynolds number (based on the freestream speed, the cross-section chord-length, and the kinematic viscosity) is set to $2000$.
The grid contains about 2.9 million cells ($1704\times1704$) with a minimum cell width of $0.004c$ in the vicinity of the cross-section.
For each test, we ran $10000$ time steps (with time-step size $0.0004$).
With used the decoupled version of the immersed-boundary projection method\cite{Li_et_al_2016}.
We used A9 instances (1, 2, 4, and 8 nodes) to run our test case on CPUs only.
The system for the velocity fluxes was solved with a stabilized bi-conjugate gradient from the PETSc library with a diagonal preconditioner.
The Poisson system was solved using a conjugate-gradient technique from the PETSc library with a classical algebraic multigrid preconditioner from Hypre BoomerAMG.
Finally, we used a direct solver of the PETSc library for the system for the Lagrangian forces.
For the iterative solvers, we set the exit criterion based on a absolute tolerance of the residuals with a value of $10^{-6}$.
We ran tests using A9 instances (1, 2, 4, and 8 nodes).
For the GPU runs, only the Poisson system was solved on GPUs and we used different instances of the NC-series, which have NVidia K80 GPU devices.


\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/runtime_azure_snake2d.png}
\caption{Wall-times for the two-dimensional flying snake example.}
\label{runtime_snake2d_azure}
\end{figure}

\subsection{Three-dimensional flying snake}


\subsubsection{Strong scaling}

We performed a similar analysis with a three-dimensional example at the same Reynolds number and angle of attack.
The snake cross-section is extruded in the third direction to get a cylindrical geometries.
The spanwise length was chosen to be $\pi c$.
In the x and y directions, the minimum grid-cell size is set $0.008c$ in the vicinity of the body  in the x-y plane

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{figures/runtime_azure_snake3d.png}
\caption{Wall-times for the three-dimensional flying snake example.}
\label{runtime_snake3d_azure}
\end{figure}

At the time of conducting this analysis, there was no InifiniBand network in place for the instances of the NC-series.
Thus, we limited our tests to a single NC24 instance (which has 24 CPU cores and 4 K80 GPU devices).

\subsubsection{Coarse mesh}

We also computed 100 non-dimensional time units of flow simulation on the mesh with 46 million cells.
The simulation ran on one instance NC24 using 24 CPU processes and 4 K80 GPU devices.

\subsubsection{Fine mesh}

The solution after 100 time units obtained with the previous mesh was then interpolated on a finer mesh (233 million cells) and ran for 90 additional time units.

We used Microsoft Azure Batch Service and Batch Shipyard to run three-dimensional computations on multiple nodes with GPU devices available and 

\section{Conclusion}

We have developed an interface between \petsc and the NVIDIA library \amgx to accelerate the time-to-solution of the Poisson system that arises from the projection method implemented in our in-house software \petibm.

\bibliography{references}

\end{document}